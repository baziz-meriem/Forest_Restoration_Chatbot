{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"cb067d9bc78645ff8e3ba1d2e418f7b0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d7d9065ea4614f17ac3a8dcf0070d058","IPY_MODEL_3fa812b2b5bc4abe99c2608b164a83aa","IPY_MODEL_a2ddfbc668a1451485acca8a7c67d7eb"],"layout":"IPY_MODEL_e67651fcdf2b4ef393a4de3ee26bc192"}},"d7d9065ea4614f17ac3a8dcf0070d058":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45426ca6e5624b81a42956667e0547a6","placeholder":"​","style":"IPY_MODEL_71dff7069b1849459f3036c95b07153b","value":"Downloading (…)-13b.ggmlv3.q5_1.bin: 100%"}},"3fa812b2b5bc4abe99c2608b164a83aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e17544c240b5457482d2fe541b2fd214","max":9763701888,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29a4b6fc83fb4eba889e59b6f269a8bd","value":9763701888}},"a2ddfbc668a1451485acca8a7c67d7eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7c527f58dbe4f60ac504bf9f82ab385","placeholder":"​","style":"IPY_MODEL_46bd78db4f22468bbac46e58ee4a49e2","value":" 9.76G/9.76G [01:18&lt;00:00, 161MB/s]"}},"e67651fcdf2b4ef393a4de3ee26bc192":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45426ca6e5624b81a42956667e0547a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71dff7069b1849459f3036c95b07153b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e17544c240b5457482d2fe541b2fd214":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29a4b6fc83fb4eba889e59b6f269a8bd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7c527f58dbe4f60ac504bf9f82ab385":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46bd78db4f22468bbac46e58ee4a49e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIIqK3P18URS","executionInfo":{"status":"ok","timestamp":1693752672098,"user_tz":-330,"elapsed":1695,"user":{"displayName":"Atreo","userId":"10523535177741183231"}},"outputId":"db3cb2b7-7046-438d-afdb-0ab7421c1350"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Sep  3 14:51:09 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   67C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"4VUyUGvLzF76"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Second Method"],"metadata":{"id":"wuLycMX9Bn7-"}},{"cell_type":"code","source":["# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n","\n","from getpass import getpass\n","\n","HUGGINGFACEHUB_API_TOKEN = getpass()"],"metadata":{"id":"AlBkz6DX_Rrf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"],"metadata":{"id":"lTKVT68J_WPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -qq langchain wget\n","!pip install gguf  #https://github.com/ggerganov/llama.cpp/tree/master/gguf-py\n","!git clone https://github.com/ggerganov/llama.cpp\n","!pip -qq install git+https://github.com/huggingface/transformers\n","!pip install --upgrade numba tensorflow\n","!pip install numpy==1.18\n","#Assuming you are using a GPU\n","!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip -qq install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"],"metadata":{"id":"LnWDeUfk8iTa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.llms import LlamaCpp\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n","\n","from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferMemory\n","\n","\n","# Callbacks support token-wise streaming\n","callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n","# Verbose is required to pass to the callback manager\n","\n","from huggingface_hub import hf_hub_download\n","repo_id=\"TheBloke/Llama-2-13B-GGML\"; filename=\"llama-2-13b.ggmlv3.q5_1.bin\""],"metadata":{"id":"TmcFQkaqMV0s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!intel-smi\n"],"metadata":{"id":"a-oHkS0TiFjC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hf_hub_download(\n","    repo_id=repo_id, filename=filename,\n","    local_dir=\"/content\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["cb067d9bc78645ff8e3ba1d2e418f7b0","d7d9065ea4614f17ac3a8dcf0070d058","3fa812b2b5bc4abe99c2608b164a83aa","a2ddfbc668a1451485acca8a7c67d7eb","e67651fcdf2b4ef393a4de3ee26bc192","45426ca6e5624b81a42956667e0547a6","71dff7069b1849459f3036c95b07153b","e17544c240b5457482d2fe541b2fd214","29a4b6fc83fb4eba889e59b6f269a8bd","b7c527f58dbe4f60ac504bf9f82ab385","46bd78db4f22468bbac46e58ee4a49e2"]},"id":"scg-SZAt96Ea","executionInfo":{"status":"ok","timestamp":1693617628826,"user_tz":300,"elapsed":78910,"user":{"displayName":"Juan Pablo Cuevas Gonzalez","userId":"14800553549127430613"}},"outputId":"9d7bae98-ca14-47a8-e07f-a077351cf24f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)-13b.ggmlv3.q5_1.bin:   0%|          | 0.00/9.76G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb067d9bc78645ff8e3ba1d2e418f7b0"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["'/content/llama-2-13b.ggmlv3.q5_1.bin'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["!python /content/llama.cpp/convert-llama-ggmlv3-to-gguf.py --input `ls -t /content/*ggmlv3*.bin | head -1` --output `ls -t /content/*ggmlv3*.bin | head -1`.gguf\n","\n","filename=filename+\".gguf\""],"metadata":{"id":"NavjrNnoB7Ux","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693617930428,"user_tz":300,"elapsed":86524,"user":{"displayName":"Juan Pablo Cuevas Gonzalez","userId":"14800553549127430613"}},"outputId":"b61aa4c2-e6e6-409c-e790-315ec06007a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["* Using config: Namespace(input=PosixPath('/content/llama-2-13b.ggmlv3.q5_1.bin'), output=PosixPath('/content/llama-2-13b.ggmlv3.q5_1.bin.gguf'), name=None, desc=None, gqa=1, eps='5.0e-06', context_length=2048, model_metadata_dir=None, vocab_dir=None, vocabtype='spm')\n","\n","=== WARNING === Be aware that this conversion script is best-effort. Use a native GGUF model if possible. === WARNING ===\n","\n","* Scanning GGML input file\n","* GGML model hyperparameters: <Hyperparameters: n_vocab=32000, n_embd=5120, n_mult=256, n_head=40, n_layer=40, n_rot=128, n_ff=13824, ftype=9>\n","\n","=== WARNING === Special tokens may not be converted correctly. Use --model-metadata-dir if possible === WARNING ===\n","\n","* Preparing to save GGUF file\n","* Adding model parameters and KV items\n","* Adding 32000 vocab item(s)\n","* Adding 363 tensor(s)\n","    gguf: write header\n","    gguf: write metadata\n","    gguf: write tensors\n","* Successful completion. Output saved to: /content/llama-2-13b.ggmlv3.q5_1.bin.gguf\n"]}]},{"cell_type":"code","source":["n_gpu_layers = 62\n","n_batch = 512\n","n_threads=4\n","llm = LlamaCpp(\n","    model_path=\"/content/\"+filename,\n","    n_threads=n_threads,\n","    n_gpu_layers=n_gpu_layers,\n","    n_batch=n_batch,\n","    callback_manager=callback_manager,\n","    n_ctx=2048,\n","    temperature=0.0,\n","    repeat_penalty=1.18,\n","    top_p=1,\n","    top_k=3,\n","    max_tokens=256,\n","    streaming=True,\n","    #verbose=True,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fvc7joPHCcQt","executionInfo":{"status":"ok","timestamp":1693617998320,"user_tz":300,"elapsed":53969,"user":{"displayName":"Juan Pablo Cuevas Gonzalez","userId":"14800553549127430613"}},"outputId":"da89f13f-82e7-4b55-b755-c5df873e3801"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"]}]},{"cell_type":"code","source":["memory = ConversationBufferMemory()\n","conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")"],"metadata":{"id":"fdQhM9I9AZiJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test CPU times: user 4min 44s, sys: 43.5 ms, total: 4min 44s n_gpu_layers = 40 n_batch = 512\n","# Wall time: 2min 59s n_gpu_layers = 40 n_batch = 812"],"metadata":{"id":"kjAtVd8-hTtn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","conversation.predict(input=\"Q: What is the restoration forest?\", stop=[\"Q:\", \"\\n\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"id":"ylL65gN8A1gM","executionInfo":{"status":"ok","timestamp":1693618207526,"user_tz":300,"elapsed":193440,"user":{"displayName":"Juan Pablo Cuevas Gonzalez","userId":"14800553549127430613"}},"outputId":"613f0f38-76b0-42d8-ca68-07be8e6d5c9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","\n","Human: Q: What is the restoration forest?\n","AI:\u001b[0m\n"," A: It's an area where trees are planted in order to restore forests that have been damaged or destroyed by human activity.\n","\u001b[1m> Finished chain.\u001b[0m\n","CPU times: user 4min 34s, sys: 4.71 ms, total: 4min 34s\n","Wall time: 3min 13s\n"]},{"output_type":"execute_result","data":{"text/plain":["\" A: It's an area where trees are planted in order to restore forests that have been damaged or destroyed by human activity.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["print(memory.buffer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WahXJHaWKhRH","executionInfo":{"status":"ok","timestamp":1693534458584,"user_tz":300,"elapsed":322,"user":{"displayName":"Juan Pablo Cuevas Gonzalez","userId":"14800553549127430613"}},"outputId":"dac16790-02d3-4ce6-c445-7b89e518445f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Human: Q: What is the restoration forest?\n","AI:  A: It's an area where trees are planted in order to restore forests that have been damaged or destroyed by human activity.\n"]}]},{"cell_type":"markdown","source":["#ConversationTokenBufferMemory"],"metadata":{"id":"WyMyxZmSehwZ"}},{"cell_type":"code","source":["from langchain.memory import ConversationTokenBufferMemory"],"metadata":{"id":"a5sdKdNNfMo9","colab":{"base_uri":"https://localhost:8080/","height":314},"executionInfo":{"status":"error","timestamp":1693752665599,"user_tz":-330,"elapsed":16,"user":{"displayName":"Atreo","userId":"10523535177741183231"}},"outputId":"b9743e73-0a11-4fd6-9212-edf9dc6bdee1"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f1523dedbdeb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversationTokenBufferMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n","\n","conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")\n","\n","memory.save_context({\"input\": \"Q: What is the restoration forest?\"},\n","                    {\"output\": \"A: It's an area where trees are planted in order to restore forests that have been damaged or destroyed by human activity.\"})\n","memory.save_context({\"input\": \"Q: Which plant or trees I should plant for make a succesfull restoration forest?\"},\n","                    {\"output\": \"A: There is no one-size-fits-all answer to this question, as the best plants and trees to use will depend on the specific conditions of your area and what you are trying to achieve. However, some general tips include choosing native species that are well adapted to local conditions, planting a variety of different types of trees to create a diverse ecosystem, and using trees that produce fruits or nuts for wildlife habitat.\"})"],"metadata":{"id":"r8mv88HyfUmg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conversation.predict(input=\"Hello\", stop=[\"Q:\", \"\\n\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"OKhvcqvou4eu","executionInfo":{"status":"ok","timestamp":1693488464518,"user_tz":300,"elapsed":155418,"user":{"displayName":"Juan Pablo Cuevas Gonzalez","userId":"14800553549127430613"}},"outputId":"fbaeb8bc-3c52-4e9b-ee78-aa5a45ed2d43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","\n","Human: Hello\n","AI:\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n"]},{"output_type":"stream","name":"stdout","text":[" Hi! I'm an artificial intelligence that can chat with you about anything. What would you like to talk about?\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\" Hi! I'm an artificial intelligence that can chat with you about anything. What would you like to talk about?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["import pickle\n","pickled_str = pickle.dumps(conversation.memory)\n","conversation2 = ConversationChain(llm=llm, memory=pickle.loads(pickled_str))"],"metadata":{"id":"PALfRsM0xE-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["memory.save_context({\"input\": \"Q: What is the restoration forest?\"},\n","                    {\"output\": \"A: It's an area where trees are planted in order to restore forests that have been damaged or destroyed by human activity.\"})\n","memory.save_context({\"input\": \"Q: Which plant or trees I should plant for make a succesfull restoration forest?\"},\n","                    {\"output\": \"A: There is no one-size-fits-all answer to this question, as the best plants and trees to use will depend on the specific conditions of your area and what you are trying to achieve. However, some general tips include choosing native species that are well adapted to local conditions, planting a variety of different types of trees to create a diverse ecosystem, and using trees that produce fruits or nuts for wildlife habitat.\"})"],"metadata":{"id":"7WxrIjU6vuV_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(memory.buffer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lfv0d65arqN-","executionInfo":{"status":"ok","timestamp":1693487923301,"user_tz":300,"elapsed":315,"user":{"displayName":"Juan Pablo Cuevas Gonzalez","userId":"14800553549127430613"}},"outputId":"aec4b231-91bf-4756-9793-d0ee26f03708"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["memory.load_memory_variables({})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4jsP8z3Zeg_O","executionInfo":{"status":"ok","timestamp":1693487931822,"user_tz":300,"elapsed":315,"user":{"displayName":"Juan Pablo Cuevas Gonzalez","userId":"14800553549127430613"}},"outputId":"428e8e5a-f784-4eea-ed5b-426a39b82a6a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': ''}"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["#ConversationSummaryMemory"],"metadata":{"id":"RasY6DNmgKVW"}},{"cell_type":"code","source":["from langchain.memory import ConversationSummaryBufferMemory"],"metadata":{"id":"qfHlRCdigKB5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a long string\n","schedule = \"A: There is no one-size-fits-all answer to this question, as the best plants and trees to use will \\\n","depend on the specific conditions of your area and what you are trying to achieve. \\\n","However, some general tips include choosing native species that are well adapted to local conditions, \\\n","planting a variety of different types of trees to create a diverse ecosystem, \\\n","and using trees that produce fruits or nuts for wildlife habitat.\"\n","\n","memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n","memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Q: What is the restoration forest?\"},\n","                    {\"output\": \"A: It's an area where trees are planted in order to restore forests that have been damaged or destroyed by human activity.\"})\n","memory.save_context({\"input\": \"Q: Which plant or trees I should plant for make a succesfull restoration forest?\"},\n","                    {\"output\": f\"{schedule}\"})"],"metadata":{"id":"9LTLrD34gdo8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["memory.load_memory_variables({})"],"metadata":{"id":"MyUP0TKzhDD8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the model\n","n_gpu_layers = 32\n","n_batch = 512\n","n_threads=4\n","llm = LlamaCpp(\n","    model_path=\"/content/\"+filename,\n","    n_threads=n_threads,\n","    n_gpu_layers=n_gpu_layers,\n","    n_batch=n_batch,\n","    callback_manager=callback_manager,\n","    n_ctx=2048,\n","    temperature=0.0,\n","    repeat_penalty=1.18,\n","    top_p=1,\n","    top_k=3,\n","    max_tokens=256,\n","    streaming=True,\n","    #verbose=True,\n",")"],"metadata":{"id":"YJKqkgOChDrD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")"],"metadata":{"id":"fPyjLWgqi6z5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conversation.predict(input=\"choosing native species is a good idea for start a restoration forest process?\")"],"metadata":{"id":"AhdLJDKvjAAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DFkFPT5WjA1a"},"execution_count":null,"outputs":[]}]}